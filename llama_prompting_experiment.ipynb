{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e3d1c2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "Few showt epreiment \n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Victorian Era Authorship Attribution with Llama3 Prompting\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------- Config & Seeding --------------------\n",
    "SEED = 42\n",
    "TEST_SIZE = 0.2\n",
    "EVAL_SAMPLES = 100\n",
    "TRAIN_CSV = 'Gungor_2018_VictorianAuthorAttribution_data-train.csv'\n",
    "AUTHOR_LIST = 'author_list.txt'  # New: file containing author names\n",
    "OLLAMA_API = \"http://localhost:11434/api\"\n",
    "OLLAMA_MODEL = \"llama3\"  # Base Ollama model\n",
    "\n",
    "# Set up enhanced logging with more descriptive information\n",
    "log_dir = \"logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = f\"{log_dir}/victorian_attribution_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"victorian_attribution\")\n",
    "\n",
    "# Create results directory\n",
    "results_dir = \"results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# -------------------- Ollama API Functions --------------------\n",
    "def check_ollama_running():\n",
    "    \"\"\"Check if Ollama is running and available\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_API}/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def call_ollama(prompt, model=OLLAMA_MODEL):\n",
    "    \"\"\"\n",
    "    Call Ollama API with improved error handling\n",
    "    - Uses stream=false to avoid JSON parsing errors\n",
    "    - Adds timeout and better error handling\n",
    "    \"\"\"\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Key fix: Setting stream to false to get a single JSON response\n",
    "            # instead of streaming multiple responses that cause parsing errors\n",
    "            response = requests.post(\n",
    "                f\"{OLLAMA_API}/generate\",\n",
    "                json={\n",
    "                    \"model\": model,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"max_tokens\": 30,\n",
    "                    \"stream\": False  # *** This is crucial to fix the JSON parsing error ***\n",
    "                },\n",
    "                timeout=30  # Add a reasonable timeout\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                try:\n",
    "                    # Parse the JSON response safely\n",
    "                    return response.json().get(\"response\", \"\").strip()\n",
    "                except (ValueError, KeyError) as json_err:\n",
    "                    logger.error(f\"JSON parsing error: {json_err}\")\n",
    "                    logger.error(f\"Response content: {response.text[:100]}...\")\n",
    "            else:\n",
    "                logger.error(f\"API error: {response.status_code} - {response.text[:100]}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Request error: {e}\")\n",
    "            \n",
    "        if attempt < max_retries - 1:\n",
    "            logger.info(f\"Retrying... Attempt {attempt+1}/{max_retries}\")\n",
    "            # More generous backoff\n",
    "            time.sleep(3 * (attempt + 1))\n",
    "        else:\n",
    "            logger.error(f\"Failed after {max_retries} attempts\")\n",
    "            return \"Error\"\n",
    "\n",
    "# -------------------- Load Author Names --------------------\n",
    "def load_author_names(author_list_path):\n",
    "    \"\"\"Load author names from author_list.txt file\"\"\"\n",
    "    logger.info(f\"Loading author names from {author_list_path}\")\n",
    "    \n",
    "    author_names = {}\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(author_list_path):\n",
    "        logger.warning(f\"Author list file {author_list_path} not found. Will use author IDs only.\")\n",
    "        return author_names\n",
    "    \n",
    "    try:\n",
    "        with open(author_list_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    # Expecting format: \"ID Name\"\n",
    "                    # E.g., \"1 Charles Dickens\"\n",
    "                    parts = line.split(maxsplit=1)\n",
    "                    if len(parts) == 2:\n",
    "                        author_id, author_name = parts\n",
    "                        try:\n",
    "                            author_id = int(author_id)\n",
    "                            author_names[author_id] = author_name\n",
    "                        except ValueError:\n",
    "                            logger.warning(f\"Invalid author ID format: {line}\")\n",
    "                    else:\n",
    "                        logger.warning(f\"Invalid author list format: {line}\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(author_names)} author names\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading author names: {e}\")\n",
    "    \n",
    "    return author_names\n",
    "\n",
    "# -------------------- Load & Create Special Split --------------------\n",
    "def load_and_map(csv_path, author_names=None):\n",
    "    \"\"\"Load CSV and map author IDs with enhanced logging\"\"\"\n",
    "    logger.info(f\"Loading Victorian author dataset from {csv_path}\")\n",
    "    df = pd.read_csv(csv_path, encoding='latin-1', engine='python')\n",
    "    if 'text' not in df.columns or 'author' not in df.columns:\n",
    "        raise KeyError(\"CSV must contain 'text' and 'author' columns\")\n",
    "    \n",
    "    # Data cleaning\n",
    "    df = df.dropna(subset=['text']).reset_index(drop=True)\n",
    "    \n",
    "    # Map author IDs to contiguous indices\n",
    "    orig_ids = sorted(df['author'].astype(int).unique())\n",
    "    id_map = {orig: idx for idx, orig in enumerate(orig_ids)}\n",
    "    df['author_idx'] = df['author'].astype(int).map(id_map)\n",
    "    \n",
    "    # Add author names to dataframe if available\n",
    "    if author_names:\n",
    "        df['author_name'] = df['author'].astype(int).map(lambda x: author_names.get(x, f\"Unknown Author {x}\"))\n",
    "        \n",
    "        # Log which author IDs don't have names\n",
    "        missing_names = [id for id in orig_ids if id not in author_names]\n",
    "        if missing_names:\n",
    "            logger.warning(f\"Missing author names for IDs: {missing_names}\")\n",
    "    else:\n",
    "        df['author_name'] = df['author'].astype(str)\n",
    "    \n",
    "    # Get author distribution\n",
    "    author_counts = df.groupby('author').size()\n",
    "    logger.info(f\"Dataset loaded: {len(df)} fragments from {len(orig_ids)} unique authors\")\n",
    "    logger.info(f\"Author distribution: Min={author_counts.min()}, Max={author_counts.max()}, Mean={author_counts.mean():.1f}\")\n",
    "    \n",
    "    return df, orig_ids\n",
    "\n",
    "# -------------------- Helper Functions --------------------\n",
    "def extract_author_id(response, valid_ids, author_names=None):\n",
    "    \"\"\"\n",
    "    Extract author ID from response text with improved pattern matching.\n",
    "    Now also checks for author names if provided.\n",
    "    \"\"\"\n",
    "    valid_ids_str = [str(id) for id in valid_ids]\n",
    "    \n",
    "    logger.debug(f\"Extracting author ID from: {response}\")\n",
    "    \n",
    "    # First look for exact matches (entire response is just a number)\n",
    "    if response in valid_ids_str:\n",
    "        logger.debug(f\"Found exact match: {response}\")\n",
    "        return response\n",
    "\n",
    "    # Look for IDs with various prefixes/patterns\n",
    "    patterns = [\n",
    "        r\"(?:author|id|author id)[:\\s]*(\\d+)\",\n",
    "        r\"(?:the answer is|i think)[:\\s]*(\\d+)\",\n",
    "        r\"(\\d+)(?:\\s*is the author)\",\n",
    "        r\"^[^\\d]*(\\d+)[^\\d]*$\"  # Improved pattern to extract a number surrounded by non-digits\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, response.lower())\n",
    "        if matches:\n",
    "            for match in matches:\n",
    "                if match in valid_ids_str:\n",
    "                    logger.debug(f\"Extracted ID {match} using pattern: {pattern}\")\n",
    "                    return match\n",
    "    \n",
    "    # If author names are provided, try to match those\n",
    "    if author_names:\n",
    "        for author_id, author_name in author_names.items():\n",
    "            # Replace common titles/honorifics that might be abbreviated differently\n",
    "            simplified_name = re.sub(r'(mr|mrs|ms|dr|sir|lady|lord)\\b\\.?', '', author_name.lower()).strip()\n",
    "            # Get last name\n",
    "            last_name = simplified_name.split()[-1] if simplified_name else \"\"\n",
    "            \n",
    "            # Create patterns to match author names\n",
    "            name_patterns = [\n",
    "                # Full name match\n",
    "                re.escape(author_name.lower()),\n",
    "                # Last name match with author context\n",
    "                rf\"(?:author|writer)[:\\s]*{re.escape(last_name)}\",\n",
    "                # Just last name if it's distinctive enough (at least 5 chars)\n",
    "                re.escape(last_name) if len(last_name) >= 5 else None\n",
    "            ]\n",
    "            \n",
    "            # Remove None patterns\n",
    "            name_patterns = [p for p in name_patterns if p]\n",
    "            \n",
    "            for pattern in name_patterns:\n",
    "                if re.search(pattern, response.lower()):\n",
    "                    logger.debug(f\"Matched author name: {author_name}\")\n",
    "                    return str(author_id)\n",
    "    \n",
    "    logger.warning(f\"Could not extract a valid author ID from: {response}\")\n",
    "    return None\n",
    "\n",
    "# -------------------- Prepare Examples for Few-Shot --------------------\n",
    "def prepare_high_quality_examples(train_df, num_authors, orig_ids, author_names=None):\n",
    "    \"\"\"Prepare high-quality, representative examples for each author\"\"\"\n",
    "    logger.info(\"Preparing carefully selected, representative examples for few-shot prompting\")\n",
    "    \n",
    "    examples = {}\n",
    "    # Get longest texts for each author to have more representative samples\n",
    "    for idx in range(num_authors):\n",
    "        author_texts = train_df[train_df['author_idx']==idx]['text'].reset_index(drop=True)\n",
    "        if len(author_texts) > 0:\n",
    "            # Get the longest example for better representation\n",
    "            text_lengths = author_texts.str.len()\n",
    "            longest_idx = text_lengths.argmax()\n",
    "            full_text = author_texts.iloc[longest_idx]\n",
    "            \n",
    "            # Limit to first 150 words for consistency\n",
    "            ex_text = ' '.join(full_text.split()[:150])\n",
    "            examples[idx] = ex_text\n",
    "            \n",
    "            # Log sample counts per author\n",
    "            author_id = orig_ids[idx]\n",
    "            author_info = f\"{author_names.get(author_id, '')} (ID: {author_id})\" if author_names else f\"Author ID {author_id}\"\n",
    "            logger.debug(f\"Selected example for {author_info} from {len(author_texts)} available samples\")\n",
    "    \n",
    "    # Save examples for future use\n",
    "    with open(f\"{results_dir}/few_shot_examples.json\", \"w\") as f:\n",
    "        json.dump({str(orig_ids[idx]): examples[idx] for idx in examples}, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"Prepared {len(examples)} high-quality examples for few-shot prompting\")\n",
    "    return examples\n",
    "\n",
    "# -------------------- Evaluation Functions --------------------\n",
    "def evaluate_zero_shot(texts, true_labels, orig_ids, author_names=None):\n",
    "    \"\"\"Evaluate zero-shot performance with Ollama - now with author names\"\"\"\n",
    "    logger.info(\"Evaluating zero-shot performance with Llama3\")\n",
    "    logger.info(\"This establishes our baseline without any examples or fine-tuning\")\n",
    "    \n",
    "    predictions = []\n",
    "    responses = []\n",
    "    times = []\n",
    "    \n",
    "    # Create author information for the prompt\n",
    "    if author_names:\n",
    "        author_info = []\n",
    "        for author_id in orig_ids:\n",
    "            author_name = author_names.get(author_id, f\"Unknown Author\")\n",
    "            author_info.append(f\"Author ID {author_id}: {author_name}\")\n",
    "        author_list = \"\\n\".join(author_info)\n",
    "    else:\n",
    "        author_list = \", \".join(str(id) for id in orig_ids)\n",
    "    \n",
    "    for i, txt in enumerate(tqdm(texts, desc=\"Zero-shot evaluation\")):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create a more informative zero-shot prompt\n",
    "        prompt = (\n",
    "            f\"Task: Victorian Era Authorship Attribution\\n\\n\"\n",
    "            f\"Instructions: Given a text fragment from the Victorian era, identify the author who wrote it. \"\n",
    "            f\"Victorian literature spans roughly from 1837 to 1901 during Queen Victoria's reign, \"\n",
    "            f\"and includes authors with distinctive writing styles, vocabulary, themes, and sentence structures.\\n\\n\"\n",
    "            f\"Available authors:\\n{author_list}\\n\\n\"\n",
    "            f\"Text fragment to analyze:\\n\\\"{txt}\\\"\\n\\n\"\n",
    "            f\"Based on the writing style, vocabulary choices, sentence structure, and thematic elements, \"\n",
    "            f\"identify which author wrote this text fragment. \"\n",
    "            f\"Respond with ONLY the numeric author ID.\"\n",
    "        )\n",
    "        \n",
    "        response = call_ollama(prompt)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        author_id = extract_author_id(response, orig_ids, author_names)\n",
    "        predictions.append(author_id if author_id else \"Unknown\")\n",
    "        responses.append(response)\n",
    "        times.append(end_time - start_time)\n",
    "        \n",
    "        if (i+1) % 10 == 0 or i == 0:\n",
    "            logger.info(f\"Zero-shot progress: {i+1}/{len(texts)} samples processed\")\n",
    "    \n",
    "    # Calculate detailed metrics\n",
    "    valid_preds = [(true, pred) for true, pred in zip(true_labels, predictions) if pred != \"Unknown\"]\n",
    "    if valid_preds:\n",
    "        true_valid = [t for t, _ in valid_preds]\n",
    "        pred_valid = [p for _, p in valid_preds]\n",
    "        accuracy = accuracy_score(true_valid, pred_valid)\n",
    "        f1 = f1_score(true_valid, pred_valid, average='weighted')\n",
    "        logger.info(f\"Zero-shot results:\")\n",
    "        logger.info(f\"  - Accuracy: {accuracy:.2%} ({len(valid_preds)}/{len(true_labels)} valid predictions)\")\n",
    "        logger.info(f\"  - F1 Score (weighted): {f1:.4f}\")\n",
    "        logger.info(f\"  - Mean response time: {np.mean(times):.2f} seconds\")\n",
    "        \n",
    "        # Save classification report\n",
    "        report = classification_report(true_valid, pred_valid, output_dict=True)\n",
    "        with open(f\"{results_dir}/zero_shot_classification_report.json\", \"w\") as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "    else:\n",
    "        accuracy = 0\n",
    "        logger.warning(\"No valid zero-shot predictions\")\n",
    "\n",
    "    # Save detailed results\n",
    "    zero_shot_results = pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'true_label': true_labels,\n",
    "        'prediction': predictions,\n",
    "        'response': responses,\n",
    "        'time': times\n",
    "    })\n",
    "    zero_shot_results.to_csv(f\"{results_dir}/zero_shot_results.csv\", index=False)\n",
    "    \n",
    "    return predictions, accuracy, zero_shot_results\n",
    "\n",
    "def evaluate_few_shot(texts, true_labels, n_shots, train_df, examples, num_authors, orig_ids, author_names=None):\n",
    "    \"\"\"Evaluate few-shot performance with carefully selected examples, now with author names\"\"\"\n",
    "    logger.info(f\"Evaluating {n_shots}-shot performance with Llama3\")\n",
    "    logger.info(f\"Using {n_shots} carefully selected examples to guide the model\")\n",
    "    \n",
    "    predictions = []\n",
    "    responses = []\n",
    "    times = []\n",
    "    per_author_correct = {str(author_id): 0 for author_id in orig_ids}\n",
    "    per_author_total = {str(author_id): 0 for author_id in orig_ids}\n",
    "\n",
    "    # Create author information for the prompt\n",
    "    if author_names:\n",
    "        author_info = []\n",
    "        for author_id in orig_ids:\n",
    "            author_name = author_names.get(author_id, f\"Unknown Author\")\n",
    "            author_info.append(f\"Author ID {author_id}: {author_name}\")\n",
    "        author_list = \"\\n\".join(author_info)\n",
    "    else:\n",
    "        author_list = \", \".join(str(id) for id in orig_ids)\n",
    "    \n",
    "    # Prepare available examples for each author\n",
    "    available_authors = list(examples.keys())\n",
    "    \n",
    "    for i, txt in enumerate(tqdm(texts, desc=f\"{n_shots}-shot evaluation\")):\n",
    "        start_time = time.time()\n",
    "        true_author = true_labels[i]\n",
    "        per_author_total[true_author] += 1\n",
    "        \n",
    "        # IMPROVED: Completely random example selection\n",
    "        # Each author has an equal chance of being included in examples\n",
    "        # No special treatment for true author (neither included nor excluded)\n",
    "        \n",
    "        # Randomly select n_shots authors from all available authors\n",
    "        selected_authors = []\n",
    "        if available_authors and len(available_authors) >= n_shots:\n",
    "            selected_authors = random.sample(available_authors, n_shots)\n",
    "        else:\n",
    "            # If we don't have enough authors, take what we can\n",
    "            selected_authors = available_authors.copy()\n",
    "            # Fill the rest with random authors (might include duplicates)\n",
    "            remaining_slots = n_shots - len(selected_authors)\n",
    "            if remaining_slots > 0 and available_authors:\n",
    "                # Allow duplicates if necessary\n",
    "                for _ in range(remaining_slots):\n",
    "                    selected_authors.append(random.choice(available_authors))\n",
    "        \n",
    "        # If we still need more examples, allow duplicates from remaining authors\n",
    "        if len(selected_authors) < n_shots:\n",
    "            remaining_slots = n_shots - len(selected_authors)\n",
    "            all_remaining = [a for a in available_authors if a not in selected_authors]\n",
    "            if all_remaining:\n",
    "                random.shuffle(all_remaining)\n",
    "                # Cycle through remaining if needed\n",
    "                for j in range(remaining_slots):\n",
    "                    selected_authors.append(all_remaining[j % len(all_remaining)])\n",
    "        \n",
    "        # Shuffle to avoid position bias\n",
    "        random.shuffle(selected_authors)\n",
    "        \n",
    "        # For each author, include an example with author name if available\n",
    "        example_texts = []\n",
    "        for auth_idx in selected_authors:\n",
    "            if auth_idx in examples:\n",
    "                author_id = orig_ids[auth_idx]\n",
    "                author_display = f\"Author ID {author_id}\"\n",
    "                if author_names and author_id in author_names:\n",
    "                    author_display = f\"{author_names[author_id]} (ID: {author_id})\"\n",
    "                \n",
    "                example_texts.append(f\"Example from {author_display}:\\n\\\"{examples[auth_idx]}\\\"\\n\")\n",
    "                \n",
    "        examples_text = \"\\n\".join(example_texts)\n",
    "\n",
    "        # Create an improved few-shot prompt with clear definitions\n",
    "        prompt = (\n",
    "            f\"Task: Victorian Era Authorship Attribution\\n\\n\"\n",
    "            f\"Instructions: Given a text fragment from the Victorian era, identify the author who wrote it. \"\n",
    "            f\"Victorian literature spans roughly from 1837 to 1901 during Queen Victoria's reign, \"\n",
    "            f\"and includes authors with distinctive writing styles, vocabulary, themes, and sentence structures.\\n\\n\"\n",
    "            f\"Here are some example text fragments with their authors:\\n\\n\"\n",
    "            f\"{examples_text}\\n\"\n",
    "            f\"Available authors:\\n{author_list}\\n\\n\"\n",
    "            f\"Text fragment to analyze:\\n\\\"{txt}\\\"\\n\\n\"\n",
    "            f\"Based on the writing style, vocabulary choices, sentence structure, and thematic elements, \"\n",
    "            f\"identify which author wrote this text fragment. Compare the writing style to the examples provided. \"\n",
    "            f\"Respond with ONLY the numeric author ID.\"\n",
    "        )\n",
    "\n",
    "        response = call_ollama(prompt)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        author_id = extract_author_id(response, orig_ids, author_names)\n",
    "        prediction = author_id if author_id else \"Unknown\"\n",
    "        predictions.append(prediction)\n",
    "        responses.append(response)\n",
    "        times.append(end_time - start_time)\n",
    "        \n",
    "        # Track per-author accuracy\n",
    "        if prediction == true_author:\n",
    "            per_author_correct[true_author] += 1\n",
    "        \n",
    "        if (i+1) % 10 == 0 or i == 0:\n",
    "            logger.info(f\"{n_shots}-shot progress: {i+1}/{len(texts)} samples processed\")\n",
    "\n",
    "    # Calculate detailed metrics\n",
    "    valid_preds = [(true, pred) for true, pred in zip(true_labels, predictions) if pred != \"Unknown\"]\n",
    "    if valid_preds:\n",
    "        true_valid = [t for t, _ in valid_preds]\n",
    "        pred_valid = [p for _, p in valid_preds]\n",
    "        accuracy = accuracy_score(true_valid, pred_valid)\n",
    "        f1 = f1_score(true_valid, pred_valid, average='weighted')\n",
    "        \n",
    "        # Calculate per-author accuracy\n",
    "        per_author_accuracy = {\n",
    "            author_id: per_author_correct[author_id]/per_author_total[author_id] \n",
    "            if per_author_total[author_id] > 0 else 0\n",
    "            for author_id in per_author_correct\n",
    "        }\n",
    "        \n",
    "        # Log detailed results\n",
    "        logger.info(f\"{n_shots}-shot results:\")\n",
    "        logger.info(f\"  - Accuracy: {accuracy:.2%} ({len(valid_preds)}/{len(true_labels)} valid predictions)\")\n",
    "        logger.info(f\"  - F1 Score (weighted): {f1:.4f}\")\n",
    "        logger.info(f\"  - Mean response time: {np.mean(times):.2f} seconds\")\n",
    "        \n",
    "        # Find authors with highest/lowest accuracy\n",
    "        if per_author_accuracy:\n",
    "            best_author_id = max(per_author_accuracy.items(), key=lambda x: x[1])[0]\n",
    "            worst_author_id = min(per_author_accuracy.items(), key=lambda x: x[1])[0]\n",
    "            \n",
    "            best_author_name = author_names.get(int(best_author_id)) if author_names else None\n",
    "            worst_author_name = author_names.get(int(worst_author_id)) if author_names else None\n",
    "            \n",
    "            if best_author_name:\n",
    "                logger.info(f\"  - Author with highest accuracy: {best_author_name} (ID: {best_author_id})\")\n",
    "            else:\n",
    "                logger.info(f\"  - Author with highest accuracy: ID {best_author_id}\")\n",
    "                \n",
    "            if worst_author_name:\n",
    "                logger.info(f\"  - Author with lowest accuracy: {worst_author_name} (ID: {worst_author_id})\")\n",
    "            else:\n",
    "                logger.info(f\"  - Author with lowest accuracy: ID {worst_author_id}\")\n",
    "        \n",
    "        # Save classification report\n",
    "        report = classification_report(true_valid, pred_valid, output_dict=True)\n",
    "        with open(f\"{results_dir}/{n_shots}_shot_classification_report.json\", \"w\") as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "            \n",
    "        # Save per-author accuracy\n",
    "        with open(f\"{results_dir}/{n_shots}_shot_per_author_accuracy.json\", \"w\") as f:\n",
    "            json.dump(per_author_accuracy, f, indent=2)\n",
    "    else:\n",
    "        accuracy = 0\n",
    "        logger.warning(f\"No valid {n_shots}-shot predictions\")\n",
    "\n",
    "    # Save detailed results\n",
    "    few_shot_results = pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'true_label': true_labels,\n",
    "        'prediction': predictions,\n",
    "        'response': responses,\n",
    "        'time': times\n",
    "    })\n",
    "    few_shot_results.to_csv(f\"{results_dir}/{n_shots}_shot_results.csv\", index=False)\n",
    "\n",
    "    return predictions, accuracy, few_shot_results\n",
    "\n",
    "# -------------------- Results Visualization --------------------\n",
    "def visualize_results(results, author_names=None):\n",
    "    \"\"\"Create enhanced visualizations for method comparison\"\"\"\n",
    "    logger.info(\"Creating comprehensive visualizations for method comparison\")\n",
    "    \n",
    "    # Create directory for visualizations\n",
    "    vis_dir = f\"{results_dir}/visualizations\"\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Method comparison bar chart with error bars\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    methods = list(results.keys())\n",
    "    accuracies = [results[m]['accuracy'] for m in methods]\n",
    "    \n",
    "    # Calculate confidence intervals (95%)\n",
    "    confidence_intervals = []\n",
    "    for method in methods:\n",
    "        if 'df' in results[method]:\n",
    "            df = results[method]['df']\n",
    "            valid_indices = df['prediction'] != \"Unknown\"\n",
    "            if sum(valid_indices) > 0:\n",
    "                true_valid = df['true_label'][valid_indices].tolist()\n",
    "                pred_valid = df['prediction'][valid_indices].tolist()\n",
    "                correct = [1 if t == p else 0 for t, p in zip(true_valid, pred_valid)]\n",
    "                n = len(correct)\n",
    "                std_err = np.std(correct, ddof=1) / np.sqrt(n) if n > 1 else 0\n",
    "                # 95% confidence interval\n",
    "                ci = 1.96 * std_err\n",
    "                confidence_intervals.append(ci)\n",
    "            else:\n",
    "                confidence_intervals.append(0)\n",
    "        else:\n",
    "            confidence_intervals.append(0)\n",
    "    \n",
    "    # Create a better bar chart with error bars\n",
    "    bars = plt.bar(methods, accuracies, yerr=confidence_intervals, \n",
    "                   capsize=10, color=plt.cm.viridis(np.linspace(0, 0.8, len(methods))),\n",
    "                   edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "    \n",
    "    plt.ylim(0, min(1.0, max(accuracies) + 0.15))\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.title('Victorian Author Attribution Methods Comparison (Llama3)', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('Accuracy', fontsize=14)\n",
    "    plt.xlabel('Method', fontsize=14)\n",
    "    plt.xticks(fontsize=12, rotation=15)\n",
    "    \n",
    "    # Add value labels with percentages\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f\"{height:.1%}\", ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add note about confidence intervals\n",
    "    plt.figtext(0.5, 0.01, \"Error bars represent 95% confidence intervals\", \n",
    "                ha=\"center\", fontsize=10, style='italic')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
    "    plt.savefig(f'{vis_dir}/method_comparison.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Per-author accuracy comparison\n",
    "    plt.figure(figsize=(14, 9))\n",
    "    \n",
    "    author_methods = {}\n",
    "    # Collect per-author accuracies for each method\n",
    "    for method in results.keys():\n",
    "        method_path = None\n",
    "        if method.endswith(\"-shot\"):\n",
    "            n_shots = method.split(\"-\")[0]\n",
    "            method_path = f\"{results_dir}/{n_shots}_shot_per_author_accuracy.json\"\n",
    "        \n",
    "        if method_path and os.path.exists(method_path):\n",
    "            with open(method_path, 'r') as f:\n",
    "                author_acc = json.load(f)\n",
    "                for author, acc in author_acc.items():\n",
    "                    if author not in author_methods:\n",
    "                        author_methods[author] = {}\n",
    "                    author_methods[author][method] = acc\n",
    "    \n",
    "    # Plot per-author comparison\n",
    "    if author_methods:\n",
    "        # Sort authors by accuracy in the best method\n",
    "        author_avg_acc = {author: np.mean(list(methods.values())) \n",
    "                         for author, methods in author_methods.items()}\n",
    "        sorted_authors = sorted(author_avg_acc.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_authors = [a[0] for a in sorted_authors[:min(10, len(sorted_authors))]]\n",
    "        \n",
    "        # Create data for plotting\n",
    "        plot_data = []\n",
    "        for author in top_authors:\n",
    "            # Add author name if available\n",
    "            author_display = author\n",
    "            if author_names:\n",
    "                try:\n",
    "                    author_id = int(author)\n",
    "                    if author_id in author_names:\n",
    "                        author_display = f\"{author_names[author_id]}\"\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            for method, acc in author_methods[author].items():\n",
    "                plot_data.append({\n",
    "                    'Author': author_display,\n",
    "                    'Author ID': author,\n",
    "                    'Method': method,\n",
    "                    'Accuracy': acc\n",
    "                })\n",
    "        \n",
    "        plot_df = pd.DataFrame(plot_data)\n",
    "        \n",
    "        # Create grouped bar chart\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        ax = sns.barplot(x='Author', y='Accuracy', hue='Method', data=plot_df, palette='viridis')\n",
    "        \n",
    "        plt.title('Per-Author Accuracy Comparison Across Methods', fontsize=16, fontweight='bold')\n",
    "        plt.ylabel('Accuracy', fontsize=14)\n",
    "        plt.xlabel('Author', fontsize=14)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.legend(title='Method', fontsize=12, title_fontsize=13)\n",
    "        \n",
    "        # Rotate x-axis labels if there are many\n",
    "        if len(top_authors) > 5:\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{vis_dir}/per_author_comparison.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # 3. Enhanced confusion matrices for the best method\n",
    "    best_method = max(results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
    "    if 'df' in results[best_method]:\n",
    "        df = results[best_method]['df']\n",
    "        valid_indices = df['prediction'] != \"Unknown\"\n",
    "        if sum(valid_indices) > 0:\n",
    "            true_valid = df['true_label'][valid_indices].tolist()\n",
    "            pred_valid = df['prediction'][valid_indices].tolist()\n",
    "            \n",
    "            # Create confusion matrix\n",
    "            unique_labels = sorted(list(set(true_valid + pred_valid)))\n",
    "            cm = confusion_matrix(true_valid, pred_valid, labels=unique_labels)\n",
    "            \n",
    "            # Normalize confusion matrix\n",
    "            cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            cm_norm = np.nan_to_num(cm_norm)  # Replace NaN with 0\n",
    "            \n",
    "            # Plot enhanced confusion matrix\n",
    "            plt.figure(figsize=(14, 12))\n",
    "            \n",
    "            # Create label mapping with author names if available\n",
    "            if author_names:\n",
    "                unique_label_names = []\n",
    "                for label in unique_labels:\n",
    "                    try:\n",
    "                        author_id = int(label)\n",
    "                        if author_id in author_names:\n",
    "                            unique_label_names.append(f\"{author_names[author_id]}\")\n",
    "                        else:\n",
    "                            unique_label_names.append(f\"ID {label}\")\n",
    "                    except:\n",
    "                        unique_label_names.append(f\"ID {label}\")\n",
    "            else:\n",
    "                unique_label_names = [f\"ID {label}\" for label in unique_labels]\n",
    "            \n",
    "            # Plot with improved aesthetics\n",
    "            sns.heatmap(cm_norm, annot=cm, fmt='d', cmap='Blues', \n",
    "                       xticklabels=unique_label_names, yticklabels=unique_label_names,\n",
    "                       linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "            \n",
    "            plt.title(f'Confusion Matrix - {best_method} Method (Best Performance)', fontsize=16, fontweight='bold')\n",
    "            plt.ylabel('True Author', fontsize=14)\n",
    "            plt.xlabel('Predicted Author', fontsize=14)\n",
    "            plt.xticks(fontsize=10, rotation=45, ha='right')\n",
    "            plt.yticks(fontsize=10)\n",
    "            \n",
    "            # Add annotations for diagonal accuracy\n",
    "            for i, label in enumerate(unique_labels):\n",
    "                if i < len(cm) and i < len(cm_norm):\n",
    "                    accuracy = cm_norm[i, i]\n",
    "                    plt.text(i+0.5, i+0.2, f\"{accuracy:.1%}\", ha='center', fontsize=9, color='darkred', fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{vis_dir}/best_method_confusion_matrix.png', dpi=300)\n",
    "            plt.close()\n",
    "    \n",
    "    # 4. Timing comparison with better visualization\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    method_times = {m: results[m]['df']['time'].mean() for m in results if 'df' in results[m]}\n",
    "    \n",
    "    methods = list(method_times.keys())\n",
    "    times = list(method_times.values())\n",
    "    \n",
    "    # Create bar chart with error bars for standard deviation\n",
    "    method_stds = {m: results[m]['df']['time'].std() for m in results if 'df' in results[m]}\n",
    "    std_times = [method_stds[m] for m in methods]\n",
    "    \n",
    "    # Use a more visually distinct color palette\n",
    "    bars = plt.bar(methods, times, yerr=std_times, capsize=10, \n",
    "                  color=plt.cm.rocket(np.linspace(0.2, 0.8, len(methods))),\n",
    "                  edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "    \n",
    "    plt.title('Average Processing Time per Sample', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('Time (seconds)', fontsize=14)\n",
    "    plt.xlabel('Method', fontsize=14)\n",
    "    plt.xticks(fontsize=12, rotation=15)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add text labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f\"{height:.2f}s\", ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{vis_dir}/timing_comparison.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Create a performance vs. efficiency scatter plot\n",
    "    if len(methods) > 1:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        accuracies = [results[m]['accuracy'] for m in methods]\n",
    "        times = [method_times[m] for m in methods]\n",
    "        \n",
    "        # Create scatter plot with sized points based on number of examples\n",
    "        sizes = []\n",
    "        for m in methods:\n",
    "            if m == \"Zero-shot\":\n",
    "                sizes.append(100)\n",
    "            elif \"shot\" in m:\n",
    "                n_shots = int(m.split(\"-\")[0])\n",
    "                sizes.append(100 + n_shots * 60)\n",
    "        \n",
    "        # Create scatter plot with custom styling\n",
    "        plt.scatter(times, accuracies, s=sizes, alpha=0.7, \n",
    "                   c=range(len(methods)), cmap='viridis', edgecolor='black', linewidth=1)\n",
    "        \n",
    "        # Add method labels to points\n",
    "        for i, method in enumerate(methods):\n",
    "            plt.annotate(method, (times[i], accuracies[i]), \n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        fontsize=11, fontweight='bold')\n",
    "        \n",
    "        plt.title('Performance vs. Efficiency Trade-off', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Average Processing Time (seconds)', fontsize=14)\n",
    "        plt.ylabel('Accuracy', fontsize=14)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add ideal region indicator (top-left corner is best - high accuracy, low time)\n",
    "        plt.axhline(y=max(accuracies), color='g', linestyle='--', alpha=0.3)\n",
    "        plt.axvline(x=min(times), color='g', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{vis_dir}/performance_vs_efficiency.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # 6. NEW: Create a chord diagram to show confusion between authors\n",
    "    if 'df' in results[best_method]:\n",
    "        try:\n",
    "            # This is optional since it requires the circlify package\n",
    "            import circlify\n",
    "            \n",
    "            df = results[best_method]['df']\n",
    "            valid_indices = df['prediction'] != \"Unknown\"\n",
    "            if sum(valid_indices) > 0:\n",
    "                true_valid = df['true_label'][valid_indices].tolist()\n",
    "                pred_valid = df['prediction'][valid_indices].tolist()\n",
    "                \n",
    "                # Create confusion matrix for top authors\n",
    "                unique_labels = sorted(list(set(true_valid + pred_valid)))\n",
    "                if len(unique_labels) > 15:\n",
    "                    # Limit to most common authors for readability\n",
    "                    label_counts = {}\n",
    "                    for label in unique_labels:\n",
    "                        label_counts[label] = true_valid.count(label) + pred_valid.count(label)\n",
    "                    top_labels = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "                    unique_labels = [l[0] for l in top_labels]\n",
    "                \n",
    "                # Create confusion data\n",
    "                confusion_data = {}\n",
    "                for true, pred in zip(true_valid, pred_valid):\n",
    "                    if true in unique_labels and pred in unique_labels:\n",
    "                        key = (true, pred)\n",
    "                        if key not in confusion_data:\n",
    "                            confusion_data[key] = 0\n",
    "                        confusion_data[key] += 1\n",
    "                \n",
    "                # Create plot data\n",
    "                if confusion_data:\n",
    "                    plt.figure(figsize=(14, 14))\n",
    "                    \n",
    "                    # Create a circular layout\n",
    "                    circles = circlify.circlify(\n",
    "                        [{'id': label, 'datum': true_valid.count(label)} for label in unique_labels],\n",
    "                        show_enclosure=False\n",
    "                    )\n",
    "                    \n",
    "                    # Plot circles\n",
    "                    ax = plt.subplot(111, aspect='equal')\n",
    "                    ax.axis('off')\n",
    "                    \n",
    "                    # Create center points dictionary\n",
    "                    centers = {}\n",
    "                    for circle in circles:\n",
    "                        label = circle.ex['id']\n",
    "                        x, y = circle.circle.center\n",
    "                        centers[label] = (x, y)\n",
    "                        r = circle.circle.radius\n",
    "                        \n",
    "                        # Get author name if available\n",
    "                        author_display = label\n",
    "                        if author_names:\n",
    "                            try:\n",
    "                                author_id = int(label)\n",
    "                                if author_id in author_names:\n",
    "                                    author_display = f\"{author_names[author_id]}\"\n",
    "                            except:\n",
    "                                pass\n",
    "                        \n",
    "                        # Draw circle\n",
    "                        ax.add_patch(plt.Circle((x, y), r, alpha=0.5, linewidth=2, \n",
    "                                              fill=True, edgecolor='black', \n",
    "                                              facecolor=plt.cm.tab20(unique_labels.index(label) % 20)))\n",
    "                        \n",
    "                        # Add text label\n",
    "                        plt.annotate(author_display, (x, y), ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "                    \n",
    "                    # Draw connections between authors\n",
    "                    for (true, pred), count in confusion_data.items():\n",
    "                        if true != pred:  # Skip self-connections\n",
    "                            # Get centers\n",
    "                            if true in centers and pred in centers:\n",
    "                                start = centers[true]\n",
    "                                end = centers[pred]\n",
    "                                \n",
    "                                # Calculate thickness based on count (normalized)\n",
    "                                max_count = max(confusion_data.values())\n",
    "                                thickness = 0.5 + 2.0 * (count / max_count)\n",
    "                                alpha = 0.3 + 0.5 * (count / max_count)\n",
    "                                \n",
    "                                # Draw arrow\n",
    "                                ax.annotate(\"\", xy=end, xytext=start,\n",
    "                                         arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=.2\", \n",
    "                                                        linewidth=thickness, alpha=alpha, color='red'))\n",
    "                    \n",
    "                    # Set axis limits\n",
    "                    ax.set_xlim(-1.2, 1.2)\n",
    "                    ax.set_ylim(-1.2, 1.2)\n",
    "                    \n",
    "                    plt.title(f'Author Confusion Network - {best_method} Method', fontsize=16, fontweight='bold')\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(f'{vis_dir}/author_confusion_network.png', dpi=300)\n",
    "                    plt.close()\n",
    "        except ImportError:\n",
    "            logger.warning(\"Circlify package not available for chord diagram. Skipping.\")\n",
    "    \n",
    "    logger.info(f\"Created enhanced visualizations in {vis_dir}\")\n",
    "\n",
    "# -------------------- Main Execution --------------------\n",
    "def main():\n",
    "    logger.info(\"======== VICTORIAN AUTHOR ATTRIBUTION WITH LLAMA3 PROMPTING ========\")\n",
    "    logger.info(\"Following professor's recommendation to try a prompting baseline\")\n",
    "    logger.info(\"This implementation ONLY tests zero-shot and few-shot prompting (NO training)\")\n",
    "    \n",
    "    # Check if Ollama is running\n",
    "    if not check_ollama_running():\n",
    "        logger.warning(\"Ollama is not running. Zero-shot and Few-shot evaluations will fail.\")\n",
    "        logger.info(\"Please start Ollama with 'ollama serve' in a separate terminal.\")\n",
    "        \n",
    "        start_ollama = input(\"Do you want to try starting Ollama now? (y/n): \")\n",
    "        if start_ollama.lower() == 'y':\n",
    "            try:\n",
    "                subprocess.Popen([\"ollama\", \"serve\"])\n",
    "                logger.info(\"Ollama started. Waiting 5 seconds for it to initialize...\")\n",
    "                time.sleep(5)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to start Ollama: {e}\")\n",
    "                logger.info(\"Please start Ollama manually and then continue.\")\n",
    "    \n",
    "    # Check if we need to pull the base Llama3 model for Ollama\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_API}/tags\")\n",
    "        if response.status_code == 200:\n",
    "            available_models = [model[\"name\"] for model in response.json()[\"models\"]]\n",
    "            if OLLAMA_MODEL not in available_models:\n",
    "                logger.warning(f\"Model {OLLAMA_MODEL} not found in Ollama. Pulling it now...\")\n",
    "                try:\n",
    "                    subprocess.run([\"ollama\", \"pull\", OLLAMA_MODEL], check=True)\n",
    "                    logger.info(f\"Successfully pulled {OLLAMA_MODEL}\")\n",
    "                except subprocess.CalledProcessError as e:\n",
    "                    logger.error(f\"Failed to pull {OLLAMA_MODEL}: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error checking Ollama models: {e}\")\n",
    "    \n",
    "    # Load author names if available\n",
    "    author_names = load_author_names(AUTHOR_LIST)\n",
    "    \n",
    "    # Load data with author names\n",
    "    logger.info(\"Starting Victorian Author Attribution with Llama3 Prompting Baseline\")\n",
    "    logger.info(\"Following professor's recommendation to establish a prompting baseline with Llama3\")\n",
    "    df_all, orig_ids = load_and_map(TRAIN_CSV, author_names)\n",
    "    num_authors = len(orig_ids)\n",
    "\n",
    "    # Split into training and test sets\n",
    "    train_df, test_df = train_test_split(\n",
    "        df_all, test_size=TEST_SIZE,\n",
    "        stratify=df_all['author_idx'], random_state=SEED\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Split sizes: Training: {len(train_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "    # Sample evaluation set from the test data\n",
    "    eval_df = test_df.sample(n=min(EVAL_SAMPLES, len(test_df)), random_state=SEED).reset_index(drop=True)\n",
    "    eval_texts = eval_df['text'].tolist()\n",
    "    eval_true_idxs = eval_df['author_idx'].tolist()\n",
    "    eval_true_labels = [str(orig_ids[idx]) for idx in eval_true_idxs]\n",
    "\n",
    "    logger.info(f\"Created evaluation set with {len(eval_texts)} samples\")\n",
    "\n",
    "    # Prepare author name information for evaluation\n",
    "    if author_names:\n",
    "        eval_author_names = [author_names.get(int(label), f\"Unknown Author {label}\") for label in eval_true_labels]\n",
    "        logger.info(f\"Using author names from {AUTHOR_LIST} for enhanced context\")\n",
    "        \n",
    "        # Print sample of authors in evaluation set\n",
    "        sample_authors = set(eval_author_names[:min(10, len(eval_author_names))])\n",
    "        logger.info(f\"Sample authors in evaluation set: {', '.join(sample_authors)}\")\n",
    "\n",
    "    # Save the data splits for reproducibility\n",
    "    logger.info(\"Saving data splits for future reproducibility\")\n",
    "    train_df.to_csv(f\"{results_dir}/train.csv\", index=False)\n",
    "    test_df.to_csv(f\"{results_dir}/test.csv\", index=False)\n",
    "    eval_df.to_csv(f\"{results_dir}/eval.csv\", index=False)\n",
    "    \n",
    "    # Prepare examples for few-shot with author names\n",
    "    examples = prepare_high_quality_examples(train_df, num_authors, orig_ids, author_names)\n",
    "    \n",
    "    # Conduct zero-shot and few-shot evaluations with base Llama3\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        # 1. Zero-shot evaluation\n",
    "        logger.info(\"\\n=== ZERO-SHOT EVALUATION ===\")\n",
    "        logger.info(\"Testing how well Llama3 can identify authors without any examples\")\n",
    "        zs_preds, zs_acc, zs_df = evaluate_zero_shot(eval_texts, eval_true_labels, orig_ids, author_names)\n",
    "        results['Zero-shot'] = {'predictions': zs_preds, 'accuracy': zs_acc, 'df': zs_df}\n",
    "        \n",
    "        # 2. Few-shot evaluations with different numbers of examples\n",
    "        shot_counts = [1, 3, 5]\n",
    "        for n in shot_counts:\n",
    "            logger.info(f\"\\n=== {n}-SHOT EVALUATION ===\")\n",
    "            logger.info(f\"Testing if providing {n} examples improves performance\")\n",
    "            fs_preds, fs_acc, fs_df = evaluate_few_shot(\n",
    "                eval_texts, eval_true_labels, n, train_df, \n",
    "                examples, num_authors, orig_ids, author_names\n",
    "            )\n",
    "            results[f\"{n}-shot\"] = {'predictions': fs_preds, 'accuracy': fs_acc, 'df': fs_df}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during prompt-based evaluations: {e}\")\n",
    "    \n",
    "    # Create comprehensive visualizations\n",
    "    visualize_results(results, author_names)\n",
    "    \n",
    "    # Save combined results\n",
    "    all_results = pd.DataFrame({\n",
    "        'text': eval_texts,\n",
    "        'true': eval_true_labels,\n",
    "    })\n",
    "    \n",
    "    if author_names:\n",
    "        all_results['true_author_name'] = [\n",
    "            author_names.get(int(label), f\"Unknown Author {label}\") \n",
    "            if label.isdigit() else \"Unknown\" \n",
    "            for label in eval_true_labels\n",
    "        ]\n",
    "    \n",
    "    for method, result in results.items():\n",
    "        all_results[method.replace('-', '_').lower()] = result['predictions']\n",
    "        \n",
    "        # Add author names for predictions where possible\n",
    "        if author_names:\n",
    "            method_col = method.replace('-', '_').lower()\n",
    "            all_results[f\"{method_col}_author_name\"] = [\n",
    "                author_names.get(int(pred), f\"Unknown Author {pred}\") \n",
    "                if pred.isdigit() else \"Unknown\" \n",
    "                for pred in result['predictions']\n",
    "            ]\n",
    "    \n",
    "    all_results.to_csv(f'{results_dir}/all_results.csv', index=False)\n",
    "    logger.info(f\"Saved detailed results to '{results_dir}/all_results.csv'\")\n",
    "    \n",
    "    # Final summary and comparison\n",
    "    logger.info(\"\\n=== VICTORIAN AUTHOR ATTRIBUTION SUMMARY ===\")\n",
    "    logger.info(\"Method Comparison:\")\n",
    "    \n",
    "    # Print results in descending order of accuracy\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
    "    for i, (method, result) in enumerate(sorted_results):\n",
    "        if i == 0:\n",
    "            logger.info(f\"  BEST METHOD: {method} - Accuracy: {result['accuracy']:.2%}\")\n",
    "        else:\n",
    "            logger.info(f\"  {method} - Accuracy: {result['accuracy']:.2%}\")\n",
    "    \n",
    "    # Compute improvement over zero-shot (if available)\n",
    "    if 'Zero-shot' in results and len(sorted_results) > 1:\n",
    "        zero_shot_acc = results['Zero-shot']['accuracy']\n",
    "        best_method, best_result = sorted_results[0]\n",
    "        \n",
    "        if best_method != 'Zero-shot':\n",
    "            improvement = (best_result['accuracy'] - zero_shot_acc) / zero_shot_acc if zero_shot_acc > 0 else float('inf')\n",
    "            logger.info(f\"\\nImprovement of best method ({best_method}) over zero-shot baseline: {improvement:.2%}\")\n",
    "    \n",
    "    # Performance analysis\n",
    "    logger.info(\"\\nPERFORMANCE ANALYSIS:\")\n",
    "    logger.info(\"- The prompting baseline approach demonstrates how well Llama3 can identify Victorian authors without any training\")\n",
    "    logger.info(\"- Using author names provides more context and potentially improves model performance\")\n",
    "    logger.info(\"- This aligns with the professor's recommendation to try a prompting baseline with a smaller LLM\")\n",
    "    \n",
    "    # Compare with traditional ML approaches (if available)\n",
    "    logger.info(\"\\nCOMPARISON WITH TRADITIONAL ML APPROACHES:\")\n",
    "    logger.info(\"- Traditional ML approaches (like in the LSTM attribution system) typically require extensive feature engineering\")\n",
    "    logger.info(\"- This prompting baseline provides a simpler alternative that requires less technical setup\")\n",
    "    logger.info(\"- The effectiveness of the prompting approach depends on the quality of examples and prompt engineering\")\n",
    "    logger.info(\"- Compare these results with your LSTM-based system to see the trade-offs between approaches\")\n",
    "    \n",
    "    logger.info(\"\\nRECOMMENDATIONS:\")\n",
    "    logger.info(\"- For best results with minimal effort, the few-shot prompting approach offers a good balance\")\n",
    "    logger.info(\"- Try experimenting with different example selection strategies to improve few-shot performance\")\n",
    "    logger.info(\"- Consider exploring hybrid approaches that combine LLM prompting with traditional ML features\")\n",
    "    \n",
    "    logger.info(\"\\n=== VICTORIAN AUTHOR ATTRIBUTION COMPLETE ===\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
